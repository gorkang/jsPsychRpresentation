---
title: "JsPsychR"
subtitle: "Open source, standard tooling for experimental protocols: towards Registered reports"
author: "**Gorka Navarrete & Herman Valencia**"
embed-resources: true
slide-number: c
output-file: "index.html"
format: 
  revealjs:
    theme: 
      - default
      - custom.scss
editor: source
echo: true
eval: false
code-fold: false
code-summary: "show code"
css: custom.css
bibliography: bibliography.bib

---

```{r setup}

```

# The past
_Gather 'round little ones_


## Old school science

::: {layout="[-10, 10]" layout-valign="bottom"}

![](img/shiny-logo.png){width=100% fig-align="right"}
:::


## Old school science

<BR> 

1) Read literature & come up with a _shiny_^[Novel, counter-intuitive, interesting,...] idea
2) Design and run experiment
3) Prepare data & **explore different analytic approaches**
4) If significant result &#8594; Write paper


## Scientific method

::: {layout="[10]" layout-valign="bottom"}
![[Source: Center for Open Science |MODIFIED|](https://www.cos.io/initiatives/registered-reports)](img/issues-scientific-method-osf_clean.png){width=80% fig-align="center" text-align="center" layout-valign="bottom"}
:::


## Some issues

::: {layout="[10]" layout-valign="bottom"}
![[Source: Center for Open Science](https://www.cos.io/initiatives/registered-reports)](img/issues-scientific-method-osf.png){width=80% fig-align="center" text-align="center" layout-valign="bottom"}
:::

## Experimenter degrees of freedom, incentives, issues

The need for significance and novelty:  

- Garden of forking paths [@rubin2017evaluation]

- p-hacking [@bruns2016p]

- Hypothesizing after the results are known (i.e. HARKing) [@kerr1998harking]

- False-positive research [@forstmeier2017detecting]

- Salami slicing [@rogers1999salami]



## The natural selection of bad science

<BR>
_The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing (...), only that publication is a principal factor for career advancement._

<BR>

_(...) in the absence of change, the existing incentives will necessarily lead to the degradation of scientific practices._ [@smaldino2016natural] ^[[https://www.youtube.com/watch?v=jdmpja67wzE](https://www.youtube.com/watch?v=jdmpja67wzE)]


## Psychology Replication crisis

Replication of 100 studies ^[From Psychological Science (PSCI), Journal of Personality and Social Psychology (JPSP), and Journal of Experimental Psychology: Learning, Memory, and Cognition (JEP: LMC)]

:::: {.columns}

::: {.column width="60%"}


**Statistically significant**:  

-  **97%** original &#8594; **36%** replications

**Effect sizes**:

- Replication effects were half the magnitude of original effects 


:::

::: {.column width="40%"}

![@open2015estimating](img/replication-crisis.png)

:::

::::


## Is there a crisis?

![@baker2016reproducibility](img/reproducibility-crisis.jpeg)


## Contributing factors


![@baker2016reproducibility](img/reproducibility-crisis2.jpeg){width=65%}


## Context

Mean number of publications for new hires in the Canadian cognitive psychology job market

::: {layout="[10]" layout-valign="bottom"}
![@pennycook2018analysis](img/job-market.png){width=80% fig-align="center" text-align="center"}
:::



## And not only Psychology


- Medicine [@ioannidis2005most]

- Cancer research [@begley2012raise]

- Finance [@bettis2012search]

- Economics [@maniadis2017replicate]

- Health informatics [@coiera2018does]

- Operations and supply chain management [@pagell2021replication]

- Methodological research [@boulesteix2020replication]

- Education [@frias2020replication]

- ...



## {background-image="img/help-run.gif"}


# Issues

# ~~Issues~~ Opportunities


## Improving Replicability

<BR>  

**Improve methods**

**Reduce errors**

**Openness**

<BR>  
@nosek2022replicability


## Improving Replicability

<BR> 

**Improve methods**

  + increase number of participants
  + better measures and manipulations
  + improve design
  + piloting


## Improving Replicability

<BR> 

**Reduce errors**

- preregistration:  

  + p-hacking
  + hypothesizing after the results are known (i.e. HARKing)
  + selective reporting

- internal replications



## Improving Replicability

<BR> 

**Openness**

- transparency of the research process  

- sharing methods, materials, procedures, and data


# Registered reports {background-image="img/unicorn.png" background-size="40%" background-position="bottom right"}

## Registered reports (RRs) {background-image="img/unicorn.png" background-size="20%" background-position="bottom right"}

<BR><BR>
_RRs were conceived to alter the incentives for authors and journals away from producing novel, positive, clean findings and towards conducting and publishing rigorous research on important questions._ @soderberg2021initial


## RRs do help {.smaller background-image="img/RRs.png" background-size="40%" background-position="right"}

:::: {.columns}


::: {.column width="60%"}


RRs **outperformed comparison papers on all 19 criteria** [@soderberg2021initial]

<BR> 

Sizable improvements in:

- rigor of methodology and analysis, and overall paper quality 

Statistically indistinguishable in:

- novelty and creativity

_RRs could improve research quality while reducing publication bias..._

:::

::: {.column width="40%"}

:::

::::


## How RRs work {background-image="img/RRs.webp" background-size="40%" background-position="bottom"}

- Write introduction, method, ... before collecting data!
- Send to journal for review
- Revise and resubmit (improve before collecting data)
- Once you get _In principle acceptance_:
  + collect data & run planned analysis
  + report results and conclusions & send for final review


## RRs advantages

- More open, preregistered, reproducible by default  

- It does not matter if p value is < 0.05   

- Less incentives for p-hacking

- No hypothesizing after the results are known (HARKing)

- More trustworthy results

::: {.fragment .highlight-red}
- You still can explore, but have to say explicitly
:::



## Registered reports are great {background-image="img/sold.png" background-size="30%" background-position="center"}


## But isn't this a bit... hard?


:::: {.columns}

::: {.column width="70%"}

- Hard to know how to analyze an experiment before having the data

- Always surprises when data arrives. How to create an analysis plan that will hold?

- Collecting ALL-THE-THINGS&#8482; allows me to figure out the best way to do analysis

:::

::: {.column width="30%"}


![](img/upside-down.png){fig-align="right"}
:::

::::


# Our path towards RRs {background-image="img/Logo-CSCN.png" background-size="30%" background-position="bottom right"}


## Background

- At the [CSCN](https://cscn.uai.cl/) (~5-10 PI's) we used different technologies for experiments: Psychopy, Qualtrics, Limesurvey, jsPsych,...  

- Each protocol started almost from scratch. A single pre-existing task would define the technology used

- Multiple implementations of the same tasks, not always exact replicas, not always easy to find

- Some would work in certain computers, other did not

- Output data wildly different, data preparation a hard task and error prone


## Issues

<BR><BR>  

- Experiments
- Resources
- Reproducibility



## Experiment issues 

-   Errors in experiment logic
-   Errors in items coding
-   Data not what we expected
-   Data structure made data preparation hard
-   Match between hypotheses and data not clear
-   Variables or questions not used in the analysis/paper


## Resources issues: projects as islands

- Time wasted re-programming tasks
- Thousands of $/€ 'invested' in licenses (e.g. Qualtrics)
- Piloting protocols as a part-time job for Research Assistants
- Time wasted re-doing data preparation (each software has its own output format) 


## Reproducibility issues

- Can I see what the participants saw in the 2012 protocol?
- Data preparation/analyses so ugly, sharing them is hard (let me clean up this a bit before sending it)
- Idiosyncratic analyses, some of which require licensed closed software (SPSS, Matlab,...)
- Location and organization of projects 
- Why is this 2012 paradigm/data analysis is not running?



## Issues Survey

:::: {.columns}

::: {.column width="30%"}

:::

::: {.column width="70%"}

2 questions survey:  

![[https://cscn.uai.cl/lab/protocols/38/](https://cscn.uai.cl/lab/protocols/38/){target="_blank"}](img/QR-survey.svg){width=400, fig-align="left"}

:::

::::

## Our wish list {background-image="img/euphoria.png" background-size="15%"}


## Our wish list {background-image="img/magic.png" background-size="30%" background-position="bottom right"}

- Open source software based on standard technologies
- Reusable tasks (my project feeds future projects)
- Based on a mature project or technologies [@de2015jspsych] ^[Psych: [https://www.jspsych.org/](https://www.jspsych.org/)]
- As many 'automagic' things as possible
- Easy to create and analyze paradigms
- Balancing participants
- Online/offline
- etc.



# The present

A few years latter...


## jsPsychR

:::: {.columns}

::: {.column width="60%"}

Open source tools to help create experimental paradigms with [jsPsych](https://www.jspsych.org), simulate participants and standardize the data preparation and analysis  
<BR>

1\) [jsPsychMaker](https://github.com/gorkang/jsPsychMaker)

2\) [jsPsychMonkeys](https://github.com/gorkang/jsPsychMonkeys)

3\) [jsPsychHelpeR](https://github.com/gorkang/jsPsychHelpeR)

:::

::: {.column width="40%"}

![](img/jsPsych-trinity.png){fig-align="center"}

:::

::::


## The tool

<BR>

A big catalog of tasks in [jsPsychMaker](https://github.com/gorkang/jsPsychMaker). Each task runs with [jsPsychMonkeys](https://github.com/gorkang/jsPsychMonkeys) to create virtual participants, and have a script in [jsPsychHelpeR](https://github.com/gorkang/jsPsychHelpeR) to automate data preparation (re-coding, reversing items, calculating dimensions, etc.)


## The goal

<BR>

The final goal is to help you **have the data preparation and analysis ready before collecting any real data**, reducing errors in the protocols, and making the move towards registered reports easier


## So far

- ~100 tasks (maker + helper)  
- Used by researchers in Chile, Colombia, Spain: 
  + \> 30 online protocols with \> 5000 participants (Prolific Academic, Social Media, etc.)  + offline
- Everything open source. \> 80 pages [manual](https://gorkang.github.io/jsPsychRmanual/)
- &#9829; **So, many, errors,** caught early &#9829;  

<!-- - 2 publications using the system (50% Registered reports) + more in the pipeline...   -->
 <!-- {.smaller} -->

## The team

Gorka Navarrete, Herman Valencia

<BR>

Initial idea and development: 

- Gorka Navarrete, Nicolas Sanchez-Fuenzalida, Nicolas Alarcón, Alejandro Cofre, Herman Valencia

Discussions, ideas, testing: 

- Esteban Hurtado, Alvaro Rivera, Juan Pablo Morales, ...


## jsPsychR

:::: {.columns}

::: {.column width="40%"}

<BR><BR>
1\) **[jsPsychMaker](https://github.com/gorkang/jsPsychMaker)**

2\) <span style="color:grey;">jsPsychMonkeys</span>

3\) <span style="color:grey;">jsPsychHelpeR</span>

:::

::: {.column width="60%"}

![](img/jsPsych-trinity.png){fig-align="center"}

:::

::::


## jsPsychMaker

![](img/jsPsychMaker.png)

## Features jsPsychMaker {.hscroll .scrollable .smaller}

- Fully open source, based on web standards ([jsPsych](https://www.jspsych.org/))
- Reuse ~ 100 tasks
- Online and offline protocols
- Balancing of participants to between participants conditions
- Easy to create new tasks 
- Full control over order or tasks (randomization, etc.)
- Participants can continue where they left (or not)
- Time and number of participants limits
- Multilingual support (for a selected number of tasks)
- All the parameters can be quickly changed editing a single file


## Available tasks {.hscroll .scrollable .smaller}

```{r}
#| eval: true
#| echo: false
#| code-fold: false

DF_raw = readr::read_csv(here::here("Presentation/data/DF_tasks.csv"))

DT::datatable(DF_raw, filter = 'top', options = list(pageLength = 100, dom = 't'), rownames = FALSE) |>
    DT::formatStyle(columns = c(1, 2, 3), fontSize = '75%')

```

## Create New Tasks

Creating new tasks is as simple as: 

1) Copy example tasks to your computer

```{r}

jsPsychMaker::copy_example_tasks(
  destination_folder = "~/Downloads/ExampleTasks", 
  which_tasks = "Slider"
  )

```

2) Copy paste items, adapt csv/excel file

```{r}
system("nautilus ~/Downloads/ExampleTasks/Slider/Slider.csv")
```


3) Create protocol (see next slide)





## Create protocol

<BR>
Create a protocol with: 

- 3 existing tasks: AIM, BNT, CRT
- the adapted Example tasks (Slider)


```{r}
jsPsychMaker::create_protocol(
  canonical_tasks = c("AIM", "BNT", "CRTMCQ4"),
  folder_tasks = "~/Downloads/ExampleTasks/",
  folder_output = "~/Downloads/protocol9996",
  launch_browser = TRUE
)
```


## jsPsychR tools

:::: {.columns}

::: {.column width="40%"}

<BR><BR>
1\) <span style="color:grey;">jsPsychMaker</span>

2\) **[jsPsychMonkeys](https://github.com/gorkang/jsPsychMonkeys)**

3\) <span style="color:grey;">jsPsychHelpeR</span>

:::

::: {.column width="60%"}

![](img/jsPsych-trinity.png){fig-align="center"}

:::

::::


## jsPsychMonkeys

::: {layout="[20,-2,5]" layout-valign="bottom"}

![](img/jsMonkeys_parallel.gif)

![](img/jsPsychMonkeys.png){fig-align="center" layout-valign="bottom"}


::::



## Features jsPsychMonkeys

- Fully open source (R, docker, selenium)
- Works for online and offline protocols
- Sequentially and in parallel
- Get pictures of each screen
- Store logs to make debugging easier
- Watch the monkeys as they work for you
- Random pauses or refreshing to simulate human behavior
- Set random seed to make the monkey's behavior predictable


## Release monkeys

Release a single Monkey and take a look:

```{r}
jsPsychMonkeys::release_the_monkeys(
  uid = 11,
  local_folder_tasks = "~/Downloads/protocol9996/",
  open_VNC = TRUE,
  wait_retry = 0
)
```

<BR>
Release 10 Monkeys in parallel:

```{r}
jsPsychMonkeys::release_the_monkeys(
  uid = 1:10,
  sequential_parallel = "parallel",
  number_of_cores = 10,
  local_folder_tasks = "~/Downloads/protocol9996/",
  open_VNC = FALSE
)
```


## jsPsychR tools

:::: {.columns}

::: {.column width="40%"}

<BR><BR>
1\) <span style="color:grey;">jsPsychMaker</span>

2\) <span style="color:grey;">jsPsychMonkeys</span>

3\) **[jsPsychHelpeR](https://github.com/gorkang/jsPsychHelpeR)**

:::

::: {.column width="60%"}

![](img/jsPsych-trinity.png){fig-align="center"}

:::

::::




## jsPsychHelpeR

![](img/jsPsychHelpeR.png)


## Features jsPsychHelpeR {.hscroll .scrollable .smaller}

- Fully open source (R) 
- Get tidy output data frames for each task, and for the whole protocol
- Standard naming for tasks, dimensions, scales, ...
- Include tests for common issues
- Snapshots to detect changes in data processing
- Functions to help create new tasks correction using a standard template
- Automatic reports with progress, descriptive statistics, code-book, etc.
- Create a fully reproducible Docker container with the project's data preparation and analysis
- Create a blinded data frame to perform blinded analyses



## jsPsychHelpeR

<BR>
Create project for data preparation:

```{r}
jsPsychHelpeR::run_initial_setup(
  pid = 9996,
  data_location = "~/Downloads/protocol9996/.data/",
  folder = "~/Downloads/jsPsychHelpeR9996", 
  dont_ask = TRUE
  )
```


## Challenge: everything in 3 minutes?

Create protocol, simulate participants and prepare data...

```{r, eval=FALSE}
rstudioapi::navigateToFile("R/script-full-process.R")
```

[Create, simulate, prepare](https://www.youtube.com/watch?v=2OXI9lzE3zU){target="_blank"}

{{< video https://www.youtube.com/watch?v=2OXI9lzE3zU width="600" height="400" >}}


## Survey Experiment Issues 

![](img/survey-issues.png){width=50% fig-align="center"}

## Survey results

Let's try to download the data, process it and show a report with the results:  

<BR>
Plan A: run Experiment Issues project

```{r}

rstudioapi::openProject("jsPsychHelpeR-ExperimentIssues/jsPsychHelpeR-ExperimentIssues.Rproj", newSession = TRUE)

```

<BR><BR>
Plan B: If something fails, we always have the monkeys!

```{r}

utils::browseURL(here::here("jsPsychHelpeR-ExperimentIssues/outputs/reports/report_analysis_monkeys.html"), 
                 browser = "firefox")

```

## {-}

![](img/perfect.png){fig-align="center"}

## Limitations

- Easy to create new scales and simple tasks, but complex experimental tasks <span style="color:red;">require javascript knowledge</span> (although there are a good number of examples available)

- Data preparation for new experimental tasks <span style="color:red;">requires expertise in R</span> (simple surveys not so much)  

- Analysis reports require some R knowledge (simple templates available)

- Needs access to a server for online tasks  

- Only behavioral tasks


# The future

 _The past is always tense, the future perfect_  

<span style="color:grey;">Zadie Smith</span> 


## {background-color="black" background-image="img/future.jpg"}

## Too many things, too little time {background-image="img/working.gif"  background-size="20%" background-position="bottom right"}

Development is linked to our needs, time and resources. Future roadmap:  

- Templates for common experimental designs (tasks, data preparation and analysis)  
- More tasks, translations, tests, ...  
- Upgrade, improve, clean, share
- jsPsychR paper  


## Help wellcome! {background-image="img/help.png" background-size="20%" background-position="bottom right"}

- Javascript programmers

- R programmers

- Documentation

- Task creators

- Testers

- Coffee brewers

- Patrons


## Back to Registered reports

<BR><BR><BR>
![](img/RRs.webp)

## Can jsPsychR really help? (1/2)

- Protocols are standardized with (mostly) clean code, open source, and based on web standards 

- Data preparation 90% automatic, standardized, beautiful 

- **Less errors** in protocols and in data preparation

- Time from idea to protocol much lower

- When errors are found and fixed, **old protocols can benefit from the corrections**, old results can be checked, ...


## Can jsPsychR really help? (2/2)

- Trivial to work on analysis **before collecting human data**

- Much easier to write up a good analysis plan, share it, improve it, ...

- Easy to create **fully reproducible** papers and results' reports

- Sharing protocol, materials, data preparation is painless (single command) ^[`usethis::use_github()`]

- Creating future-proof fully reproducible data preparation projects (with Docker) is one command away ^[`jsPsychHelpeR::create_docker_container()`]



## jsPsychR &#9829; Registered Reports

![](img/reproducibility-crisis2_solved.jpeg){width=65% fig-align="center"}

## More information

RRs' templates, checklists, participating journals (\> 300):  

- [https://www.cos.io/initiatives/registered-reports](www.cos.io/initiatives/registered-reports)


Also, check out the future:  

- RRs v2: [Peer community in](https://peercommunityin.org/)

And if you are a reviewer:

- No reviews unless data, stimuli and materials are publicly available ([www.opennessinitiative.org/](www.opennessinitiative.org/))


## References

::: {#refs}
:::


# Thanks!

<span style="color:grey;">Gorka Navarrete</span>

gorkang\@gmail.com  

::: {layout="[-30, 10]" layout-valign="bottom"}

![](img/QR-presentation.svg){width=70% fig-align="right"}

:::

::: footer

Presentation: [https://gorkang.github.io/jsPsychRpresentation/](https://gorkang.github.io/jsPsychRpresentation/)  
Manual: [https://gorkang.github.io/jsPsychRmanual/](https://gorkang.github.io/jsPsychRmanual/)  
Contact: [https://fosstodon.org/\@gorkang](https://fosstodon.org/\@gorkang)
:::
